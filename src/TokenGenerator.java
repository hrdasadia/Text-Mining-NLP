import java.io.BufferedReader;
import java.io.FileReader;
import java.util.ArrayList;
import java.util.Collections;
import java.util.List;

import javax.xml.parsers.DocumentBuilder;
import javax.xml.parsers.DocumentBuilderFactory;
import javax.xml.xpath.XPath;
import javax.xml.xpath.XPathConstants;
import javax.xml.xpath.XPathExpression;
import javax.xml.xpath.XPathFactory;

import org.w3c.dom.Document;
import org.w3c.dom.NodeList;

public class TokenGenerator {
/*
 * STEP 3: This method accepts all XML docs generated by Stanford NLP from step 2 and creates a list of tokens by parsing through lemma tags
 * 
 */
	public static List<String> generateTokens(List<String> xmlDocs)	throws Exception {
		List<String> tokenList = new ArrayList<String>();
		List<String> stopWords = getStopWords();
		for (int j = 0; j < xmlDocs.size(); j++) {
			DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();
			factory.setNamespaceAware(true);
			DocumentBuilder builder = factory.newDocumentBuilder();
			// parse the xml to extract term from lemma tags
			Document doc = builder.parse("C:/Users/Hardik/workspace/Text Mining/Processed Dataset/processed_document_"+ j + ".xml");
			XPathFactory xpathfactory = XPathFactory.newInstance();
			XPath xpath = xpathfactory.newXPath();
			XPathExpression expr = xpath.compile("//lemma/text()");
			Object result = expr.evaluate(doc, XPathConstants.NODESET);
			NodeList nodes = (NodeList) result;
			StringBuilder sb = new StringBuilder("");
			for (int i = 0; i < nodes.getLength(); i++) {
				if (Collections.binarySearch(stopWords, nodes.item(i)
						.getNodeValue()) < 0) {
					sb.append(nodes.item(i).getNodeValue());
					sb.append(" ");
				}
			}
			tokenList.add(sb.toString());
		}
		tokenList = filterTokens(tokenList);
		return tokenList;
	}

	/*
	 * This helper method filters the unnecessary tokens
	 */
	public static List<String> filterTokens(List<String> tokenList) {
		ArrayList<String> filteredTokens = new ArrayList<String>();
		for (String token : tokenList) {
			token = token.replaceAll("[&$-+.^:,'`\"<>/]", "");
			token = token.replaceAll("[!-~]?\\b[\\w]\\b[!-~]?", "");
			token = token.replaceAll("\\\\", "");
			filteredTokens.add(token + " ");
		}
		return filteredTokens;
	}

	/*
	 * This helper method helps identify stop words from a file and prevent creating incorrect useless tokens
	 */
	public static List<String> getStopWords() throws Exception {
		ArrayList<String> stopWords = new ArrayList<String>();
		BufferedReader br = new BufferedReader(new FileReader(
				"C:/Users/Hardik/workspace/Text Mining/Dataset/stop.txt"));
		String stopWord = "";
		while ((stopWord = br.readLine()) != null) {
			stopWords.add(stopWord);
		}
		br.close();
		return stopWords;
	}
}
